# My AI Chat

Next.jsとAI SDKを使用してOllamaと連携する高機能チャットアプリケーションです。  
LLMモデル管理、ダーク/ライトテーマ、チャット履歴管理など、豊富な機能を提供します。

## 機能

- 🤖 **LLMモデル管理**: ダウンロード、インストール、アンインストール
- 🌓 **ダーク/ライトテーマ**: お好みのテーマでご利用いただけます
- 💾 **チャット履歴管理**: 複数のチャットスレッドを管理
- 📊 **メタデータ表示**: トークン数、レスポンス時間など
- 📱 **レスポンシブデザイン**: モバイル・デスクトップ対応
- 🔄 **リアルタイムストリーミング**: AIからの即座な応答
- ⚙️ **Ollama サーバー切り替え**: アプリ内で複数のOllamaサーバーを動的に切り替え

## 必要な環境

- Docker & Docker Compose

## 🚀 簡単デプロイ

### 自動デプロイスクリプト使用（推奨）

1. リポジトリをクローン
```bash
git clone https://github.com/Takashi-Matsumura/my-ai-chat.git
cd my-ai-chat
```

2. デプロイスクリプトを実行
```bash
./deploy.sh
```

このスクリプトが自動的に以下を実行します：
- 既存コンテナの停止・削除
- 新しいイメージのビルド
- コンテナの起動
- 動作確認

### 手動デプロイ

1. リポジトリをクローン
```bash
git clone https://github.com/Takashi-Matsumura/my-ai-chat.git
cd my-ai-chat
```

2. Dockerコンテナを起動
```bash
docker-compose up -d
```

3. アプリケーションにアクセス  
   ブラウザで `http://localhost:65000` を開いてチャットを開始できます。

## 🎯 アクセス先

- **チャットアプリ**: http://localhost:65000
- **Ollama API**: http://localhost:65434
- **モデル管理**: http://localhost:65000/settings

### ローカル開発環境でのセットアップ

1. 依存関係をインストール
```bash
npm install
```

2. 開発サーバーを起動
```bash
npm run dev
```

3. ブラウザで `http://localhost:3000` を開く

## Docker管理コマンド

```bash
# コンテナの起動
docker-compose up -d

# コンテナの停止
docker-compose down

# ログの確認
docker-compose logs

# コンテナの状態確認
docker-compose ps

# Ollamaモデルの管理
docker exec my-ai-chat-ollama-1 ollama list        # インストール済みモデル
docker exec my-ai-chat-ollama-1 ollama rm <model>  # モデル削除
```

## 利用可能なコマンド

- `npm run dev` - 開発サーバー起動 (Next.js)
- `npm run build` - プロダクション用ビルド
- `npm run start` - プロダクションサーバー起動
- `npm run lint` - ESLintチェック

## 技術スタック

- **フレームワーク**: Next.js 14 (App Router)
- **言語**: TypeScript
- **スタイリング**: Tailwind CSS
- **AI統合**: AI SDK (@ai-sdk/react, @ai-sdk/openai)
- **LLMプロバイダー**: Ollama
- **デプロイ**: Docker & Docker Compose

## アーキテクチャ

### コア構造
- **フロントエンド**: React + AI SDK (`@ai-sdk/react`) の`useChat`フックでストリーミングチャットインターフェース
- **バックエンド**: Next.js API Route (`/api/chat`) でローカルOllamaからのストリーミング応答を処理
- **スタイリング**: Tailwind CSS（最小限のカスタムスタイリング）
- **AI統合**: Ollama（`http://localhost:11434/v1`）に接続する`@ai-sdk/openai`を使用

### 重要なファイル
- `app/page.tsx` - `useChat`フックを使用するメインチャットインターフェース
- `app/api/chat/route.ts` - OllamaへのリクエストをプロキシするAPIエンドポイント
- `app/layout.tsx` - Interフォントとメタデータを含むルートレイアウト

### チャットフロー
1. フロントエンドの`useChat`フックでユーザー入力を処理
2. `/api/chat`エンドポイントにメッセージを送信
3. バックエンドが`streamText`を使用してOllamaからの応答をストリーミング
4. フロントエンドがローディング状態とエラーハンドリングでストリーミング応答を表示

## ⚙️ 設定

### ポート設定
- **Next.jsアプリ**: 65000
- **Ollama API**: 65434 (外部アクセス) / 11434 (コンテナ内部)
- **Ollama URL**: `http://ollama:11434` (コンテナ間通信)

### Ollamaサーバー設定

#### 動的サーバー切り替え機能
アプリケーション内で複数のOllamaサーバーを動的に切り替えることができます。

- **設定場所**: モデル管理画面（/settings）の「サーバー設定」ボタン
- **デフォルト**: `http://localhost:11434`
- **設定保存**: ブラウザのlocalStorageに永続化
- **接続テスト**: 設定前にサーバーの動作確認が可能

#### 使用例
- **PC上のOllama**: `http://localhost:11434`
- **Docker内のOllama**: `http://localhost:11435`
- **リモートサーバー**: `http://192.168.1.100:11434`

#### 環境変数（非推奨）
**注意**: 環境変数`OLLAMA_URL`を設定すると、アプリ内の動的サーバー切り替え機能が無効化されます。  
動的切り替えを使用したい場合は、`.env.local`ファイルを削除するか、`OLLAMA_URL`変数をコメントアウトしてください。

### モデル管理
アプリケーション内のモデル管理画面（/settings）から以下が可能です：
- 🔽 **ダウンロード**: 15以上のモデルから選択
- 📦 **インストール**: バックグラウンド処理で通知付き
- 🗑️ **アンインストール**: 不要なモデルの削除

### 推奨モデル
- **軽量**: `gemma2:2b` (1.5GB)
- **日本語対応**: `dsasai/llama3-elyza-jp-8b` (4.6GB)
- **高性能**: `mistral:latest` (4.1GB)

## 💾 データの永続性とバックアップ

### チャットデータの保存場所
チャットデータは **ブラウザのlocalStorage** に保存されます。データベースは使用していません。

### データが失われるタイミング

#### 🚨 **完全にデータが失われる場合**
- **ブラウザのデータクリア**: キャッシュ・Cookie削除時
- **ブラウザ設定**: 「サイトデータを削除」実行時
- **プライベートモード**: シークレットモード使用時（終了時に削除）
- **ブラウザ・OS操作**: アンインストール、OS初期化
- **システム障害**: ハードディスククラッシュ、ファイル破損

#### ⚠️ **データが見えなくなる場合**
- **異なるブラウザ使用**: Chrome ↔ Safari等の切り替え
- **異なるデバイス使用**: PC ↔ スマートフォン等
- **プライベートモード使用**: 通常モードのデータが見えない

### データの永続性

#### ✅ **保持されます**
- 通常のブラウザ使用
- ページリロード・ブラウザ再起動
- PC再起動・アプリ更新
- サーバー再起動・ネットワーク切断

### 🔄 バックアップ機能
重要なチャットを失わないため、以下の機能を提供しています：

- **📤 エクスポート**: チャットデータをJSONファイルでダウンロード
- **📥 インポート**: バックアップファイルから復元
- **🔗 アクセス**: 設定画面（/settings）から利用可能

**推奨**: 重要なチャットは定期的にエクスポートしてバックアップを取ることをお勧めします。